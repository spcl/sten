{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sten\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from the dense implementation of $d = (a + b) c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10, 20, requires_grad=True)\n",
    "b = torch.randn(10, 20, requires_grad=True)\n",
    "c = torch.randn(20, 30, requires_grad=True)\n",
    "grad_d = torch.randn(10, 30)\n",
    "\n",
    "d = torch.mm(torch.add(a, b), c)\n",
    "d.backward(grad_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define custom random fraction sparsifier functioning the same as `sten.RandomFractionSparsifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomFractionSparsifier:\n",
    "    def __init__(self, fraction):\n",
    "        self.fraction = fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then declare tensor in CSC format that will utilize scipy CSC implementation under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCscTensor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dense(tensor):\n",
    "        return MyCscTensor(scipy.sparse.csc_matrix(tensor))\n",
    "\n",
    "    def to_dense(self):\n",
    "        return torch.from_numpy(self.data.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the result of addition $a + b$ sparse. To achieve this, we need to replace addition operator by its sparse counterpart. For simplicity, we do not use inline sparsifier, that's why operator outputs dense `torch.Tensor` after applying `KeepAll` sparsifier. We use external random fraction sparsifier with 0.5 dropout probability and output tensor in the newly defined CSC format. The same specification is assigned to the gradient format, but nothing prevents us from applying different sparsifier and useing different format for the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_add = sten.sparsified_op(\n",
    "    orig_op=torch.add,\n",
    "    out_fmt=(\n",
    "        (sten.KeepAll(), torch.Tensor, MyRandomFractionSparsifier(0.5), MyCscTensor),\n",
    "    ),\n",
    "    grad_out_fmt=(\n",
    "        (sten.KeepAll(), torch.Tensor, MyRandomFractionSparsifier(0.5), MyCscTensor),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try to use the operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Sparse operator implementation is not registered (fwd). op: <built-in method add of type object at 0x7f033a216ea0> inp: (<class 'torch.Tensor'>, <class 'torch.Tensor'>, None, None) out: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>),).. Fallback to dense implementation.\n",
      "WARNING:root:Sparsifier implementation is not registered. sparsifier: <class '__main__.MyRandomFractionSparsifier'> inp: <class 'torch.Tensor'> out: <class '__main__.MyCscTensor'>. Fallback to dense keep all implementation.\n",
      "WARNING:root:Sparse operator implementation is not registered (fwd). op: <built-in method mm of type object at 0x7f033a216ea0> inp: (<class '__main__.MyCscTensor'>, <class 'torch.Tensor'>) out: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>),).. Fallback to dense implementation.\n"
     ]
    }
   ],
   "source": [
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first error message indicates the operator implementation which is required is not registered. Here we register it and try calling the method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Sparsifier implementation is not registered. sparsifier: <class '__main__.MyRandomFractionSparsifier'> inp: <class 'torch.Tensor'> out: <class '__main__.MyCscTensor'>. Fallback to dense keep all implementation.\n",
      "WARNING:root:Sparse operator implementation is not registered (fwd). op: <built-in method mm of type object at 0x7f033a216ea0> inp: (<class '__main__.MyCscTensor'>, <class 'torch.Tensor'>) out: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>),).. Fallback to dense implementation.\n"
     ]
    }
   ],
   "source": [
    "@sten.register_fwd_op_impl(\n",
    "    operator=torch.add,\n",
    "    inp=(torch.Tensor, torch.Tensor, None, None),\n",
    "    out=tuple([(sten.KeepAll, torch.Tensor)]),\n",
    ")\n",
    "def sparse_add_fwd_impl(ctx, inputs, output_sparsifiers):\n",
    "    input, other, alpha, out = inputs\n",
    "    return torch.add(input, other, alpha=alpha, out=out)\n",
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that sparsifier implementation is not registered. Let's provide it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Sparse operator implementation is not registered (fwd). op: <built-in method mm of type object at 0x7f033a216ea0> inp: (<class '__main__.MyCscTensor'>, <class 'torch.Tensor'>) out: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>),).. Fallback to dense implementation.\n"
     ]
    }
   ],
   "source": [
    "@sten.register_sparsifier_implementation(\n",
    "    sparsifer=MyRandomFractionSparsifier, inp=torch.Tensor, out=MyCscTensor\n",
    ")\n",
    "def scalar_fraction_sparsifier_dense_coo(sparsifier, tensor):\n",
    "    return sten.SparseTensorWrapper(\n",
    "        MyCscTensor.from_dense(\n",
    "            sten.random_mask_sparsify(tensor, frac=sparsifier.fraction)\n",
    "        )\n",
    "    )\n",
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $a + b$ is sparse now and it is used as an input of `torch.mm`, we need to provide sparse operator implementation for it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sten.register_fwd_op_impl(\n",
    "    operator=torch.mm,\n",
    "    inp=(MyCscTensor, torch.Tensor),\n",
    "    out=tuple([(sten.KeepAll, torch.Tensor)]),\n",
    ")\n",
    "def torch_mm_fwd_impl(ctx, inputs, output_sparsifiers):\n",
    "    input1, input2 = inputs\n",
    "    ctx.save_for_backward(input1, input2)\n",
    "    output = torch.from_numpy(input1.wrapped_tensor.data @ input2.numpy())\n",
    "    return output\n",
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it works. The next step is to call backward pass and see what is remaining to be implemented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse operator implementation is not registered (bwd). op: <built-in method mm of type object at 0x7f033a216ea0> grad_out: (<class 'torch.Tensor'>,) grad_inp: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>), (<class 'sten.KeepAll'>, <class 'torch.Tensor'>)) inp: (<class '__main__.MyCscTensor'>, <class 'torch.Tensor'>).\n"
     ]
    }
   ],
   "source": [
    "d = torch.mm(sparse_add(a, b), c)\n",
    "try:\n",
    "    d.backward(grad_d)\n",
    "except sten.DispatchError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering backward implementation for `torch.mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse operator implementation is not registered (bwd). op: <built-in method add of type object at 0x7f033a216ea0> grad_out: (<class '__main__.MyCscTensor'>,) grad_inp: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>), (<class 'sten.KeepAll'>, <class 'torch.Tensor'>), None, None) inp: (<class 'torch.Tensor'>, <class 'torch.Tensor'>, None, None).\n"
     ]
    }
   ],
   "source": [
    "@sten.register_bwd_op_impl(\n",
    "    operator=torch.mm,\n",
    "    grad_out=(torch.Tensor,),\n",
    "    grad_inp=(\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "    ),\n",
    "    inp=(MyCscTensor, torch.Tensor),\n",
    ")\n",
    "def torch_mm_bwd_impl(ctx, grad_outputs, input_sparsifiers):\n",
    "    input1, input2 = ctx.saved_tensors\n",
    "    [grad_output] = grad_outputs\n",
    "    grad_input1 = torch.mm(grad_output, input2.T)\n",
    "    grad_input2 = torch.from_numpy(input1.wrapped_tensor.data.transpose() @ grad_output)\n",
    "    return grad_input1, grad_input2\n",
    "\n",
    "d = torch.mm(sparse_add(a, b), c)\n",
    "try:\n",
    "    d.backward(grad_d)\n",
    "except sten.DispatchError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward implementation for `torch.add`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sten.register_bwd_op_impl(\n",
    "    operator=torch.add,\n",
    "    grad_out=(MyCscTensor,),\n",
    "    grad_inp=(\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        None,\n",
    "        None,\n",
    "    ),\n",
    "    inp=(torch.Tensor, torch.Tensor, None, None),\n",
    ")\n",
    "def torch_add_bwd_impl(ctx, grad_outputs, input_sparsifiers):\n",
    "    [grad_output] = grad_outputs\n",
    "    dense_output = grad_output.wrapped_tensor.to_dense()\n",
    "    return dense_output, dense_output, None, None\n",
    "\n",
    "d = torch.mm(sparse_add(a, b), c)\n",
    "d.backward(grad_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now backward pass is also fully functional."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db772567c4567f4b3d1f56ed53e5d7229d15382f6d3dfec57e0c55f7b5cf2dd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
