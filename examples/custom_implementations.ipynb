{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the API to register custom operator implementations for specific input and output tensor formats. This example demonstrates customization API to define new sparse tensor formats and sparsifier. It shows how to register custom operator and sparsifier implementations for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/aivanov/miniconda3/envs/sten1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sten\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from the dense implementation of $d = (a + b) c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10, 20, requires_grad=True)\n",
    "b = torch.randn(10, 20, requires_grad=True)\n",
    "c = torch.randn(20, 30, requires_grad=True)\n",
    "grad_d = torch.randn(10, 30)\n",
    "\n",
    "d = torch.mm(torch.add(a, b), c)\n",
    "d.backward(grad_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a custom random fraction sparsifier functioning the same as `sten.RandomFractionSparsifier`. The sparsifier implementation is not defined here since it is characterized not only by the sparsifier itself but also by the input and output tensor formats. The sparsifier class only needs to declare its configurable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomFractionSparsifier:\n",
    "    def __init__(self, fraction):\n",
    "        self.fraction = fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then declare a tensor in CSC format that will utilize scipy CSC implementation under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCscTensor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def to_dense(self):\n",
    "        return torch.from_numpy(self.data.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make the result of addition $a + b$ sparse. To achieve this, we need to replace the addition operator with its sparse counterpart. For simplicity, we do not use an inline sparsifier, which is why the operator outputs a dense `torch.Tensor` after applying the `KeepAll` sparsifier. We use an external random fraction sparsifier with 0.5 dropout probability and output the tensor in the newly defined CSC format. The same specification is assigned to the gradient format, but nothing prevents us from applying a different sparsifier and using a different format for the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_add = sten.sparsified_op(\n",
    "    orig_op=torch.add,\n",
    "    out_fmt=(\n",
    "        (sten.KeepAll(), torch.Tensor,\n",
    "         MyRandomFractionSparsifier(0.5), MyCscTensor),\n",
    "    ),\n",
    "    grad_out_fmt=(\n",
    "        (sten.KeepAll(), torch.Tensor,\n",
    "         MyRandomFractionSparsifier(0.5), MyCscTensor),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try to use the operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsifier implementation is not registered:\n",
      "@sten.register_sparsifier_implementation(\n",
      "    sparsifier=MyRandomFractionSparsifier, inp=torch.Tensor, out=MyCscTensor\n",
      ")\n",
      "def my_sparsifier_implementation(sparsifier, tensor, grad_fmt=None):\n",
      "    return sparsified_tensor_wrapper\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    d = torch.mm(sparse_add(a, b), c)\n",
    "except sten.DispatchError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that sparsifier implementation is not registered. Let's provide it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/aivanov/sten1/src/sten/sten.py:1428: DispatchWarning: Sparse operator implementation is not registered (fwd):\n",
      "@sten.register_fwd_op_impl(\n",
      "    operator=torch.spmm,\n",
      "    inp=(MyCscTensor, torch.Tensor),  \n",
      "    out=None,  # default (dense)\n",
      ")\n",
      "def my_operator(ctx, inputs, output_sparsifiers):\n",
      "    return outputs\n",
      "Fallback to dense implementation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@sten.register_sparsifier_implementation(\n",
    "    sparsifier=MyRandomFractionSparsifier, inp=torch.Tensor, out=MyCscTensor\n",
    ")\n",
    "def torch_tensor_to_csc_random_fraction(sparsifier, tensor, grad_fmt=None):\n",
    "    return sten.SparseTensorWrapper.wrapped_from_dense(\n",
    "        MyCscTensor(scipy.sparse.csc_matrix(sten.random_mask_sparsify(tensor, sparsifier.fraction))),\n",
    "        tensor,\n",
    "        grad_fmt,\n",
    "    )\n",
    "    \n",
    "try:\n",
    "    d = torch.mm(sparse_add(a, b), c)\n",
    "except sten.DispatchError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $a + b$ is sparse now and it is used as an input of `torch.mm`, we need to provide sparse operator implementation for it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sten.register_fwd_op_impl(\n",
    "    operator=torch.mm,\n",
    "    inp=(MyCscTensor, torch.Tensor),\n",
    "    out=[(sten.KeepAll, torch.Tensor)],\n",
    ")\n",
    "def torch_mm_fwd_impl(ctx, inputs, output_sparsifiers):\n",
    "    input1, input2 = inputs\n",
    "    ctx.save_for_backward(input1, input2)\n",
    "    output = torch.from_numpy(input1.wrapped_tensor.data @ input2.numpy())\n",
    "    return output\n",
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it works. The next step is to call the backward pass and see what remains to be implemented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse operator implementation is not registered (bwd):\n",
      "@sten.register_bwd_op_impl(\n",
      "    operator=torch.spmm,\n",
      "    grad_out=None,  # default (dense)\n",
      "    grad_inp=None,  # default (dense)\n",
      "    inp=(MyCscTensor, torch.Tensor),  \n",
      ")\n",
      "def my_operator(ctx, grad_outputs, input_sparsifiers):\n",
      "    return grad_inputs\n"
     ]
    }
   ],
   "source": [
    "d = torch.mm(sparse_add(a, b), c)\n",
    "try:\n",
    "    d.backward(grad_d)\n",
    "except sten.DispatchError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering the backward implementation for `torch.mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sten.register_bwd_op_impl(\n",
    "    operator=torch.mm,\n",
    "    grad_out=[torch.Tensor],\n",
    "    grad_inp=(\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "    ),\n",
    "    inp=(MyCscTensor, torch.Tensor),\n",
    ")\n",
    "def torch_mm_bwd_impl(ctx, grad_outputs, input_sparsifiers):\n",
    "    input1, input2 = ctx.saved_tensors\n",
    "    [grad_output] = grad_outputs\n",
    "    grad_input1 = torch.mm(grad_output, input2.T)\n",
    "    grad_input2 = torch.from_numpy(\n",
    "        input1.wrapped_tensor.data.transpose() @ grad_output)\n",
    "    return grad_input1, grad_input2\n",
    "\n",
    "d = torch.mm(sparse_add(a, b), c)\n",
    "d.backward(grad_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now backward pass is also fully functional."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sten1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faa923315ecf15f4653530526b0d3c584ef5f4f6244f0a40d7cbffbaf96f5375"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
