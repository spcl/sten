{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how to build a sparse model from scratch using a simple MLP.\n",
    "As reference we use the following implementation of a dense MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, channel_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        in_out_pairs = list(zip(channel_sizes[:-1], channel_sizes[1:]))\n",
    "        for idx, (in_channels, out_channels) in enumerate(in_out_pairs):\n",
    "            if idx != 0:\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Linear(in_channels, out_channels))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "\n",
    "\n",
    "model = MLP([50, 40, 30, 20, 30, 10])\n",
    "output = model(torch.randn(15, 50))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to replace `torch.nn.Linear` with our custom `SparseLinear` module, which will call our sparse implementation of `torch.nn.functional.linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/aivanov/miniconda3/envs/sten1/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sten\n",
    "\n",
    "class SparseLinear(torch.nn.Module):\n",
    "    def __init__(self, input_features, output_features, weight_sparsity):\n",
    "        super().__init__()\n",
    "        self.weight_sparsity = weight_sparsity\n",
    "        self.weight = sten.SparseParameterWrapper(\n",
    "            sten.random_fraction_sparsifier_dense_csc(\n",
    "                sten.RandomFractionSparsifier(self.weight_sparsity),\n",
    "                torch.randn(output_features, input_features),\n",
    "                (\n",
    "                    sten.KeepAll(),\n",
    "                    torch.Tensor,\n",
    "                    sten.RandomFractionSparsifier(self.weight_sparsity),\n",
    "                    sten.CscTensor,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter(torch.rand(output_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        sparse_op = sten.sparsified_op(\n",
    "            orig_op=torch.nn.functional.linear,\n",
    "            out_fmt=tuple(\n",
    "                [(sten.KeepAll(), torch.Tensor,\n",
    "                  sten.KeepAll(), torch.Tensor)]\n",
    "            ),\n",
    "            grad_out_fmt=tuple(\n",
    "                [(sten.KeepAll(), torch.Tensor,\n",
    "                  sten.KeepAll(), torch.Tensor)]\n",
    "            ),\n",
    "        )\n",
    "        return sparse_op(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important aspect is the use of `SparseParameterWrapper` to hold the data of sparse tensors. The code above shows the sparsity configuration of weight and intermediate tensors gradients that will appear in the backward pass, although they are dense in this example. The remaining piece is the implementation of `SparseMLP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMLP(torch.nn.Module):\n",
    "    def __init__(self, channel_sizes, weight_sparsity):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        in_out_pairs = list(zip(channel_sizes[:-1], channel_sizes[1:]))\n",
    "        for idx, (in_channels, out_channels) in enumerate(in_out_pairs):\n",
    "            if idx != 0:\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(SparseLinear(\n",
    "                in_channels, out_channels, weight_sparsity))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after the replacement of `torch.nn.Linear` with the `SparseLinear` in the `MLP` implementation, we call it and observe the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "model = SparseMLP([50, 40, 30, 20, 30, 10], 0.8)\n",
    "output = model(torch.randn(15, 50))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sten1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faa923315ecf15f4653530526b0d3c584ef5f4f6244f0a40d7cbffbaf96f5375"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
