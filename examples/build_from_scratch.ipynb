{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how to build sparse model from scratch using a simple MLP.\n",
    "In the process we also explain how user can provide tensors in their own formats.\n",
    "As reference we use the following implementation of a dense MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, channel_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        in_out_pairs = list(zip(channel_sizes[:-1], channel_sizes[1:]))\n",
    "        for idx, (in_channels, out_channels) in enumerate(in_out_pairs):\n",
    "            if idx != 0:\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Linear(in_channels, out_channels))\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "model = MLP([50, 40, 30, 20, 30, 10])\n",
    "output = model(torch.randn(15, 50))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to replace `torch.nn.Linear` with our custom `SparseLinear` module, which will call our sparse implementation of `torch.nn.functional.linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sten\n",
    "class SparseLinear(torch.nn.Module):\n",
    "    def __init__(self, input_features, output_features, weight_sparsity):\n",
    "        super().__init__()\n",
    "        self.weight_sparsity = weight_sparsity\n",
    "        dense_weight = sten.random_mask_sparsify(\n",
    "            torch.randn(output_features, input_features), frac=weight_sparsity\n",
    "        )\n",
    "        self.weight = sten.SparseParameterWrapper(\n",
    "            sten.CscTensor.from_dense(dense_weight)\n",
    "        )\n",
    "        self.weight.grad_fmt = (\n",
    "            sten.KeepAll(),\n",
    "            torch.Tensor,\n",
    "            sten.RandomFractionSparsifier(self.weight_sparsity),\n",
    "            sten.CscTensor,\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter(torch.rand(output_features))\n",
    "        self.bias.grad_fmt = (\n",
    "            sten.KeepAll(),\n",
    "            torch.Tensor,\n",
    "            sten.KeepAll(),\n",
    "            torch.Tensor,\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        sparse_op = sten.sparsified_op(\n",
    "            orig_op=torch.nn.functional.linear,\n",
    "            out_fmt=tuple(\n",
    "                [(sten.KeepAll(), torch.Tensor, sten.KeepAll(), torch.Tensor)]\n",
    "            ),\n",
    "            grad_out_fmt=tuple(\n",
    "                [(sten.KeepAll(), torch.Tensor, sten.KeepAll(), torch.Tensor)]\n",
    "            ),\n",
    "        )\n",
    "        return sparse_op(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important aspect is the use of `SparseParameterWrapper` to hold the data of sparse tensors. The code above shows the sparsity configuration of weight and intermediate tensors gradients that will appear in the backward pass, although they are dense in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMLP(torch.nn.Module):\n",
    "    def __init__(self, channel_sizes, weight_sparsity):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        in_out_pairs = list(zip(channel_sizes[:-1], channel_sizes[1:]))\n",
    "        for idx, (in_channels, out_channels) in enumerate(in_out_pairs):\n",
    "            if idx != 0:\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(SparseLinear(in_channels, out_channels, weight_sparsity))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after the replacement of `torch.nn.Linear` with the `SparseLinear` in the `MLP` implementation, we call it and observe the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "model = SparseMLP([50, 40, 30, 20, 30, 10], 0.8)\n",
    "output = model(torch.randn(15, 50))\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ff2a09b934b25be9a8b176e9d1e80ece927d491d3847c62b1a0ae23e4142515"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
