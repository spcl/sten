{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sten\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from the dense implementation of $d = (a + b) c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10, 20, requires_grad=True)\n",
    "b = torch.randn(10, 20, requires_grad=True)\n",
    "c = torch.randn(20, 30, requires_grad=True)\n",
    "grad_d = torch.randn(10, 30)\n",
    "\n",
    "d = torch.mm(torch.add(a, b), c)\n",
    "d.backward(grad_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define custom random fraction sparsifier functioning the same as `sten.RandomFractionSparsifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomFractionSparsifier:\n",
    "    def __init__(self, fraction):\n",
    "        self.fraction = fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then declare tensor in CSC format that will utilize scipy CSC implementation under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCscTensor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dense(tensor):\n",
    "        return MyCscTensor(scipy.sparse.csc_matrix(tensor))\n",
    "\n",
    "    def to_dense(self):\n",
    "        return torch.from_numpy(self.data.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the result of addition $a + b$ sparse. To achieve this, we need to replace addition operator by its sparse counterpart. For simplicity, we do not use inline sparsifier, that's why operator outputs dense `torch.Tensor` after applying `KeepAll` sparsifier. We use external random fraction sparsifier with 0.5 dropout probability and output tensor in the newly defined CSC format. The same specification is assigned to the gradient format, but nothing prevents us from applying different sparsifier and useing different format for the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_add = sten.sparsified_op(\n",
    "    orig_op=torch.add,\n",
    "    out_fmt=(\n",
    "        (sten.KeepAll(), torch.Tensor, MyRandomFractionSparsifier(0.5), MyCscTensor),\n",
    "    ),\n",
    "    grad_out_fmt=(\n",
    "        (sten.KeepAll(), torch.Tensor, MyRandomFractionSparsifier(0.5), MyCscTensor),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try to use the operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Sparse operator implementation is not registered (fwd). op: <built-in method add of type object at 0x7f19d6216ea0> inp: (<class 'torch.Tensor'>, <class 'torch.Tensor'>, None, None) out: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>),)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/user/code/sten/custom_implementations.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000016?line=0'>1</a>\u001b[0m d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(sparse_add(a, b), c)\n",
      "File \u001b[0;32m~/code/sten/sten.py:576\u001b[0m, in \u001b[0;36msparsified_op.<locals>.sparse_op\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=572'>573</a>\u001b[0m \u001b[39m# arguments in the order of definition\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=573'>574</a>\u001b[0m flat_args \u001b[39m=\u001b[39m bound_args\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=575'>576</a>\u001b[0m outputs \u001b[39m=\u001b[39m SparseOperatorDispatcher\u001b[39m.\u001b[39;49mapply(orig_op, out_fmt, \u001b[39m*\u001b[39;49mflat_args)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=576'>577</a>\u001b[0m outputs \u001b[39m=\u001b[39m canonicalize_tensor_tuple(outputs)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=577'>578</a>\u001b[0m \u001b[39mfor\u001b[39;00m out, grad_fmt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(outputs, grad_out_fmt):\n",
      "File \u001b[0;32m~/code/sten/sten.py:535\u001b[0m, in \u001b[0;36mSparseOperatorDispatcher.forward\u001b[0;34m(ctx, orig_op, out_fmt, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=532'>533</a>\u001b[0m sp1, fmt1, sp2, fmt2 \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mout_fmt))\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=533'>534</a>\u001b[0m op_out_fmt \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m((s\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, f) \u001b[39mfor\u001b[39;00m s, f \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sp1, fmt1))\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=534'>535</a>\u001b[0m op_impl \u001b[39m=\u001b[39m get_fwd_op_impl(orig_op, ctx\u001b[39m.\u001b[39;49minp_fmt, op_out_fmt)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=535'>536</a>\u001b[0m tmp_outputs \u001b[39m=\u001b[39m op_impl(ctx, args, sp1)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=536'>537</a>\u001b[0m tmp_outputs \u001b[39m=\u001b[39m canonicalize_tensor_tuple(tmp_outputs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:435\u001b[0m, in \u001b[0;36mget_fwd_op_impl\u001b[0;34m(operator, inp, out)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=432'>433</a>\u001b[0m impl \u001b[39m=\u001b[39m FWD_OP_IMPLS\u001b[39m.\u001b[39mget((operator, inp, out))\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=433'>434</a>\u001b[0m \u001b[39mif\u001b[39;00m impl \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=434'>435</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=435'>436</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSparse operator implementation is not registered (fwd). op: \u001b[39m\u001b[39m{\u001b[39;00moperator\u001b[39m}\u001b[39;00m\u001b[39m inp: \u001b[39m\u001b[39m{\u001b[39;00minp\u001b[39m}\u001b[39;00m\u001b[39m out: \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=436'>437</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=437'>438</a>\u001b[0m \u001b[39mreturn\u001b[39;00m impl\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Sparse operator implementation is not registered (fwd). op: <built-in method add of type object at 0x7f19d6216ea0> inp: (<class 'torch.Tensor'>, <class 'torch.Tensor'>, None, None) out: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>),)\""
     ]
    }
   ],
   "source": [
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message indicates the operator implementation which is required is not registered. Here we register it and try calling the method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Sparsifier implementation is not registered. sparsifier: <class '__main__.MyRandomFractionSparsifier'> inp: <class 'torch.Tensor'> out: <class '__main__.MyCscTensor'>\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/user/code/sten/custom_implementations.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000018?line=6'>7</a>\u001b[0m     \u001b[39minput\u001b[39m, other, alpha, out \u001b[39m=\u001b[39m inputs\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000018?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39madd(\u001b[39minput\u001b[39m, other, alpha\u001b[39m=\u001b[39malpha, out\u001b[39m=\u001b[39mout)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000018?line=8'>9</a>\u001b[0m d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(sparse_add(a, b), c)\n",
      "File \u001b[0;32m~/code/sten/sten.py:576\u001b[0m, in \u001b[0;36msparsified_op.<locals>.sparse_op\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=572'>573</a>\u001b[0m \u001b[39m# arguments in the order of definition\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=573'>574</a>\u001b[0m flat_args \u001b[39m=\u001b[39m bound_args\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=575'>576</a>\u001b[0m outputs \u001b[39m=\u001b[39m SparseOperatorDispatcher\u001b[39m.\u001b[39;49mapply(orig_op, out_fmt, \u001b[39m*\u001b[39;49mflat_args)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=576'>577</a>\u001b[0m outputs \u001b[39m=\u001b[39m canonicalize_tensor_tuple(outputs)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=577'>578</a>\u001b[0m \u001b[39mfor\u001b[39;00m out, grad_fmt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(outputs, grad_out_fmt):\n",
      "File \u001b[0;32m~/code/sten/sten.py:540\u001b[0m, in \u001b[0;36mSparseOperatorDispatcher.forward\u001b[0;34m(ctx, orig_op, out_fmt, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=537'>538</a>\u001b[0m check_formats(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00morig_op\u001b[39m}\u001b[39;00m\u001b[39m (fwd)\u001b[39m\u001b[39m\"\u001b[39m, tmp_outputs, fmt1)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=538'>539</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(tmp_outputs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(out_fmt)\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=539'>540</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=540'>541</a>\u001b[0m     get_sparsifier_implementation(s2\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, f1, f2)(s2, tmp_out)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=541'>542</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m tmp_out, f1, s2, f2 \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(tmp_outputs, fmt1, sp2, fmt2)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=542'>543</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=543'>544</a>\u001b[0m \u001b[39mreturn\u001b[39;00m simplify_tensor_tuple(outputs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:541\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=537'>538</a>\u001b[0m check_formats(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00morig_op\u001b[39m}\u001b[39;00m\u001b[39m (fwd)\u001b[39m\u001b[39m\"\u001b[39m, tmp_outputs, fmt1)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=538'>539</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(tmp_outputs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(out_fmt)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=539'>540</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=540'>541</a>\u001b[0m     get_sparsifier_implementation(s2\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, f1, f2)(s2, tmp_out)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=541'>542</a>\u001b[0m     \u001b[39mfor\u001b[39;00m tmp_out, f1, s2, f2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tmp_outputs, fmt1, sp2, fmt2)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=542'>543</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=543'>544</a>\u001b[0m \u001b[39mreturn\u001b[39;00m simplify_tensor_tuple(outputs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:408\u001b[0m, in \u001b[0;36mget_sparsifier_implementation\u001b[0;34m(sparsifier, inp, out)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=405'>406</a>\u001b[0m impl \u001b[39m=\u001b[39m SPARSIFIER_IMPLEMENTATIONS\u001b[39m.\u001b[39mget((sparsifier, inp, out))\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=406'>407</a>\u001b[0m \u001b[39mif\u001b[39;00m impl \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=407'>408</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=408'>409</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSparsifier implementation is not registered. sparsifier: \u001b[39m\u001b[39m{\u001b[39;00msparsifier\u001b[39m}\u001b[39;00m\u001b[39m inp: \u001b[39m\u001b[39m{\u001b[39;00minp\u001b[39m}\u001b[39;00m\u001b[39m out: \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=409'>410</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=410'>411</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=411'>412</a>\u001b[0m \u001b[39mreturn\u001b[39;00m impl\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Sparsifier implementation is not registered. sparsifier: <class '__main__.MyRandomFractionSparsifier'> inp: <class 'torch.Tensor'> out: <class '__main__.MyCscTensor'>\""
     ]
    }
   ],
   "source": [
    "@sten.register_fwd_op_impl(\n",
    "    operator=torch.add,\n",
    "    inp=(torch.Tensor, torch.Tensor, None, None),\n",
    "    out=tuple([(sten.KeepAll, torch.Tensor)]),\n",
    ")\n",
    "def sparse_add_fwd_impl(ctx, inputs, output_sparsifiers):\n",
    "    input, other, alpha, out = inputs\n",
    "    return torch.add(input, other, alpha=alpha, out=out)\n",
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that sparsifier implementation is not registered. Let's provide it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Sparse operator implementation is not registered (fwd). op: <built-in method mm of type object at 0x7f19d6216ea0> inp: (<class '__main__.MyCscTensor'>, <class 'torch.Tensor'>) out: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>),)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/user/code/sten/custom_implementations.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=0'>1</a>\u001b[0m \u001b[39m@sten\u001b[39m\u001b[39m.\u001b[39mregister_sparsifier_implementation(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=1'>2</a>\u001b[0m     sparsifer\u001b[39m=\u001b[39mMyRandomFractionSparsifier, inp\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mTensor, out\u001b[39m=\u001b[39mMyCscTensor\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscalar_fraction_sparsifier_dense_coo\u001b[39m(sparsifier, tensor):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m sten\u001b[39m.\u001b[39mSparseTensorWrapper(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=5'>6</a>\u001b[0m         MyCscTensor\u001b[39m.\u001b[39mfrom_dense(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=6'>7</a>\u001b[0m             sten\u001b[39m.\u001b[39mrandom_mask_sparsify(tensor, frac\u001b[39m=\u001b[39msparsifier\u001b[39m.\u001b[39mfraction)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=7'>8</a>\u001b[0m         )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=8'>9</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000017?line=9'>10</a>\u001b[0m d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmm(sparse_add(a, b), c)\n",
      "File \u001b[0;32m~/code/sten/sten.py:60\u001b[0m, in \u001b[0;36mSparseTensorWrapper.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/user/code/sten/sten.py?line=55'>56</a>\u001b[0m         funcself \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///home/user/code/sten/sten.py?line=56'>57</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     <a href='file:///home/user/code/sten/sten.py?line=57'>58</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFunction is not explicitly added into DEFAULT_FUNCTIONS: \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mfuncself\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///home/user/code/sten/sten.py?line=58'>59</a>\u001b[0m     )\n\u001b[0;32m---> <a href='file:///home/user/code/sten/sten.py?line=59'>60</a>\u001b[0m \u001b[39mreturn\u001b[39;00m HANDLED_FUNCTIONS[func](\n\u001b[1;32m     <a href='file:///home/user/code/sten/sten.py?line=60'>61</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__, func, types, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     <a href='file:///home/user/code/sten/sten.py?line=61'>62</a>\u001b[0m )\n",
      "File \u001b[0;32m~/code/sten/sten.py:108\u001b[0m, in \u001b[0;36msparse_operator_dispatch\u001b[0;34m(base_impl, func, types, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=100'>101</a>\u001b[0m \u001b[39m@implements\u001b[39m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlinear, torch\u001b[39m.\u001b[39mmm)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msparse_operator_dispatch\u001b[39m(base_impl, func, types, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=102'>103</a>\u001b[0m     op \u001b[39m=\u001b[39m sparsified_op(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=103'>104</a>\u001b[0m         func,\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=104'>105</a>\u001b[0m         \u001b[39mtuple\u001b[39m([(KeepAll(), torch\u001b[39m.\u001b[39mTensor, KeepAll(), torch\u001b[39m.\u001b[39mTensor)]),\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=105'>106</a>\u001b[0m         \u001b[39mtuple\u001b[39m([(KeepAll(), torch\u001b[39m.\u001b[39mTensor, KeepAll(), torch\u001b[39m.\u001b[39mTensor)]),\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=106'>107</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=107'>108</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:576\u001b[0m, in \u001b[0;36msparsified_op.<locals>.sparse_op\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=572'>573</a>\u001b[0m \u001b[39m# arguments in the order of definition\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=573'>574</a>\u001b[0m flat_args \u001b[39m=\u001b[39m bound_args\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=575'>576</a>\u001b[0m outputs \u001b[39m=\u001b[39m SparseOperatorDispatcher\u001b[39m.\u001b[39;49mapply(orig_op, out_fmt, \u001b[39m*\u001b[39;49mflat_args)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=576'>577</a>\u001b[0m outputs \u001b[39m=\u001b[39m canonicalize_tensor_tuple(outputs)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=577'>578</a>\u001b[0m \u001b[39mfor\u001b[39;00m out, grad_fmt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(outputs, grad_out_fmt):\n",
      "File \u001b[0;32m~/code/sten/sten.py:535\u001b[0m, in \u001b[0;36mSparseOperatorDispatcher.forward\u001b[0;34m(ctx, orig_op, out_fmt, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=532'>533</a>\u001b[0m sp1, fmt1, sp2, fmt2 \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mout_fmt))\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=533'>534</a>\u001b[0m op_out_fmt \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m((s\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, f) \u001b[39mfor\u001b[39;00m s, f \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sp1, fmt1))\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=534'>535</a>\u001b[0m op_impl \u001b[39m=\u001b[39m get_fwd_op_impl(orig_op, ctx\u001b[39m.\u001b[39;49minp_fmt, op_out_fmt)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=535'>536</a>\u001b[0m tmp_outputs \u001b[39m=\u001b[39m op_impl(ctx, args, sp1)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=536'>537</a>\u001b[0m tmp_outputs \u001b[39m=\u001b[39m canonicalize_tensor_tuple(tmp_outputs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:435\u001b[0m, in \u001b[0;36mget_fwd_op_impl\u001b[0;34m(operator, inp, out)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=432'>433</a>\u001b[0m impl \u001b[39m=\u001b[39m FWD_OP_IMPLS\u001b[39m.\u001b[39mget((operator, inp, out))\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=433'>434</a>\u001b[0m \u001b[39mif\u001b[39;00m impl \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=434'>435</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=435'>436</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSparse operator implementation is not registered (fwd). op: \u001b[39m\u001b[39m{\u001b[39;00moperator\u001b[39m}\u001b[39;00m\u001b[39m inp: \u001b[39m\u001b[39m{\u001b[39;00minp\u001b[39m}\u001b[39;00m\u001b[39m out: \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=436'>437</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=437'>438</a>\u001b[0m \u001b[39mreturn\u001b[39;00m impl\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Sparse operator implementation is not registered (fwd). op: <built-in method mm of type object at 0x7f19d6216ea0> inp: (<class '__main__.MyCscTensor'>, <class 'torch.Tensor'>) out: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>),)\""
     ]
    }
   ],
   "source": [
    "@sten.register_sparsifier_implementation(\n",
    "    sparsifer=MyRandomFractionSparsifier, inp=torch.Tensor, out=MyCscTensor\n",
    ")\n",
    "def scalar_fraction_sparsifier_dense_coo(sparsifier, tensor):\n",
    "    return sten.SparseTensorWrapper(\n",
    "        MyCscTensor.from_dense(\n",
    "            sten.random_mask_sparsify(tensor, frac=sparsifier.fraction)\n",
    "        )\n",
    "    )\n",
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $a + b$ is sparse now and it is used as an input of `torch.mm`, we need to provide sparse operator implementation for it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sten.register_fwd_op_impl(\n",
    "    operator=torch.mm,\n",
    "    inp=(MyCscTensor, torch.Tensor),\n",
    "    out=tuple([(sten.KeepAll, torch.Tensor)]),\n",
    ")\n",
    "def torch_mm_fwd_impl(ctx, inputs, output_sparsifiers):\n",
    "    input1, input2 = inputs\n",
    "    ctx.save_for_backward(input1, input2)\n",
    "    output = torch.from_numpy(input1.wrapped_tensor.data @ input2.numpy())\n",
    "    return output\n",
    "d = torch.mm(sparse_add(a, b), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it works. The next step is to call backward pass and see what is remaining to be implemented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Sparse operator implementation is not registered (bwd). op: <built-in method mm of type object at 0x7f19d6216ea0> grad_out: (<class 'torch.Tensor'>,) grad_inp: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>), (<class 'sten.KeepAll'>, <class 'torch.Tensor'>)) inp: (<class '__main__.MyCscTensor'>, <class 'torch.Tensor'>)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/user/code/sten/custom_implementations.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000025?line=0'>1</a>\u001b[0m d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(sparse_add(a, b), c)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000025?line=1'>2</a>\u001b[0m d\u001b[39m.\u001b[39;49mbackward(grad_d)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=248'>249</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=249'>250</a>\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=250'>251</a>\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=251'>252</a>\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=252'>253</a>\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:553\u001b[0m, in \u001b[0;36mSparseOperatorDispatcher.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=548'>549</a>\u001b[0m grad_out_fmt \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(get_format(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=549'>550</a>\u001b[0m op_inp_fmt \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=550'>551</a>\u001b[0m     ((s\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, f) \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m s, f \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sp1, fmt1)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=551'>552</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=552'>553</a>\u001b[0m op_impl \u001b[39m=\u001b[39m get_bwd_op_impl(ctx\u001b[39m.\u001b[39;49morig_op, grad_out_fmt, op_inp_fmt, ctx\u001b[39m.\u001b[39;49minp_fmt)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=553'>554</a>\u001b[0m tmp_grad_inputs \u001b[39m=\u001b[39m op_impl(ctx, args, sp1)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=554'>555</a>\u001b[0m tmp_grad_inputs \u001b[39m=\u001b[39m canonicalize_tensor_tuple(tmp_grad_inputs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:455\u001b[0m, in \u001b[0;36mget_bwd_op_impl\u001b[0;34m(operator, grad_out, grad_inp, inp)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=452'>453</a>\u001b[0m impl \u001b[39m=\u001b[39m BWD_OP_IMPLS\u001b[39m.\u001b[39mget((operator, grad_out, grad_inp, inp))\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=453'>454</a>\u001b[0m \u001b[39mif\u001b[39;00m impl \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=454'>455</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=455'>456</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSparse operator implementation is not registered (bwd). op: \u001b[39m\u001b[39m{\u001b[39;00moperator\u001b[39m}\u001b[39;00m\u001b[39m grad_out: \u001b[39m\u001b[39m{\u001b[39;00mgrad_out\u001b[39m}\u001b[39;00m\u001b[39m grad_inp: \u001b[39m\u001b[39m{\u001b[39;00mgrad_inp\u001b[39m}\u001b[39;00m\u001b[39m inp: \u001b[39m\u001b[39m{\u001b[39;00minp\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=456'>457</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=457'>458</a>\u001b[0m \u001b[39mreturn\u001b[39;00m impl\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Sparse operator implementation is not registered (bwd). op: <built-in method mm of type object at 0x7f19d6216ea0> grad_out: (<class 'torch.Tensor'>,) grad_inp: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>), (<class 'sten.KeepAll'>, <class 'torch.Tensor'>)) inp: (<class '__main__.MyCscTensor'>, <class 'torch.Tensor'>)\""
     ]
    }
   ],
   "source": [
    "d = torch.mm(sparse_add(a, b), c)\n",
    "d.backward(grad_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering backward implementation for `torch.mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Sparse operator implementation is not registered (bwd). op: <built-in method add of type object at 0x7f19d6216ea0> grad_out: (<class '__main__.MyCscTensor'>,) grad_inp: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>), (<class 'sten.KeepAll'>, <class 'torch.Tensor'>), None, None) inp: (<class 'torch.Tensor'>, <class 'torch.Tensor'>, None, None)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/user/code/sten/custom_implementations.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000026?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_input1, grad_input2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000026?line=16'>17</a>\u001b[0m d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(sparse_add(a, b), c)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/code/sten/custom_implementations.ipynb#ch0000026?line=17'>18</a>\u001b[0m d\u001b[39m.\u001b[39;49mbackward(grad_d)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=248'>249</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=249'>250</a>\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=250'>251</a>\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=251'>252</a>\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> <a href='file:///home/user/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py?line=252'>253</a>\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:553\u001b[0m, in \u001b[0;36mSparseOperatorDispatcher.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=548'>549</a>\u001b[0m grad_out_fmt \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(get_format(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=549'>550</a>\u001b[0m op_inp_fmt \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=550'>551</a>\u001b[0m     ((s\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, f) \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m s, f \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sp1, fmt1)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=551'>552</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=552'>553</a>\u001b[0m op_impl \u001b[39m=\u001b[39m get_bwd_op_impl(ctx\u001b[39m.\u001b[39;49morig_op, grad_out_fmt, op_inp_fmt, ctx\u001b[39m.\u001b[39;49minp_fmt)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=553'>554</a>\u001b[0m tmp_grad_inputs \u001b[39m=\u001b[39m op_impl(ctx, args, sp1)\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=554'>555</a>\u001b[0m tmp_grad_inputs \u001b[39m=\u001b[39m canonicalize_tensor_tuple(tmp_grad_inputs)\n",
      "File \u001b[0;32m~/code/sten/sten.py:455\u001b[0m, in \u001b[0;36mget_bwd_op_impl\u001b[0;34m(operator, grad_out, grad_inp, inp)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=452'>453</a>\u001b[0m impl \u001b[39m=\u001b[39m BWD_OP_IMPLS\u001b[39m.\u001b[39mget((operator, grad_out, grad_inp, inp))\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=453'>454</a>\u001b[0m \u001b[39mif\u001b[39;00m impl \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/user/code/sten/sten.py?line=454'>455</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=455'>456</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSparse operator implementation is not registered (bwd). op: \u001b[39m\u001b[39m{\u001b[39;00moperator\u001b[39m}\u001b[39;00m\u001b[39m grad_out: \u001b[39m\u001b[39m{\u001b[39;00mgrad_out\u001b[39m}\u001b[39;00m\u001b[39m grad_inp: \u001b[39m\u001b[39m{\u001b[39;00mgrad_inp\u001b[39m}\u001b[39;00m\u001b[39m inp: \u001b[39m\u001b[39m{\u001b[39;00minp\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=456'>457</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user/code/sten/sten.py?line=457'>458</a>\u001b[0m \u001b[39mreturn\u001b[39;00m impl\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Sparse operator implementation is not registered (bwd). op: <built-in method add of type object at 0x7f19d6216ea0> grad_out: (<class '__main__.MyCscTensor'>,) grad_inp: ((<class 'sten.KeepAll'>, <class 'torch.Tensor'>), (<class 'sten.KeepAll'>, <class 'torch.Tensor'>), None, None) inp: (<class 'torch.Tensor'>, <class 'torch.Tensor'>, None, None)\""
     ]
    }
   ],
   "source": [
    "@sten.register_bwd_op_impl(\n",
    "    operator=torch.mm,\n",
    "    grad_out=(torch.Tensor,),\n",
    "    grad_inp=(\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "    ),\n",
    "    inp=(MyCscTensor, torch.Tensor),\n",
    ")\n",
    "def torch_mm_bwd_impl(ctx, grad_outputs, input_sparsifiers):\n",
    "    input1, input2 = ctx.saved_tensors\n",
    "    [grad_output] = grad_outputs\n",
    "    grad_input1 = torch.mm(grad_output, input2.T)\n",
    "    grad_input2 = torch.from_numpy(input1.wrapped_tensor.data.transpose() @ grad_output)\n",
    "    return grad_input1, grad_input2\n",
    "\n",
    "d = torch.mm(sparse_add(a, b), c)\n",
    "d.backward(grad_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward implementation for `torch.add`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sten.register_bwd_op_impl(\n",
    "    operator=torch.add,\n",
    "    grad_out=(MyCscTensor,),\n",
    "    grad_inp=(\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        None,\n",
    "        None,\n",
    "    ),\n",
    "    inp=(torch.Tensor, torch.Tensor, None, None),\n",
    ")\n",
    "def torch_add_bwd_impl(ctx, grad_outputs, input_sparsifiers):\n",
    "    [grad_output] = grad_outputs\n",
    "    dense_output = grad_output.wrapped_tensor.to_dense()\n",
    "    return dense_output, dense_output, None, None\n",
    "\n",
    "d = torch.mm(sparse_add(a, b), c)\n",
    "d.backward(grad_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now backward pass is also fully functional."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db772567c4567f4b3d1f56ed53e5d7229d15382f6d3dfec57e0c55f7b5cf2dd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
